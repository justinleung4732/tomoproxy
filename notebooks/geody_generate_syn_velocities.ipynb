{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some General Libraries \n",
    "import numpy as np\n",
    "import pyshtools as shtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweaking $PYTHONPATH so we find TOMOPROXY\n",
    "import sys\n",
    "import os, glob\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tomoproxy modules\n",
    "import tomoproxy.plotting\n",
    "import tomoproxy.sh_tools\n",
    "import tomoproxy.geody_compare as gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Compositions, pPv Scenarios and Mineralogical Models\n",
    "comp_list = ['pyrolite', 'BMO', 'MORB', 'HC']\n",
    "ppv_list = ['noppv', 'ppv', 'partppv']\n",
    "min_model_list = ['SLB_2022', 'SLB_2011']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input file\n",
    "input_path = '../data/D12_models/'\n",
    "\n",
    "# depth\n",
    "depth = np.loadtxt(input_path + 'D12_4000_depth_layers.dat')[:,1]\n",
    "depth = depth[14:] # Truncate upper mantle layers\n",
    "\n",
    "# coordinates\n",
    "coords = np.loadtxt(input_path + 'D12_4000_lonlat.dat')\n",
    "lon = coords[:,0] % 360\n",
    "lat = coords[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and create Layers\n",
    "i = 1\n",
    "\n",
    "t_grid = {'TC': np.zeros((len(depth),len(lat))),\n",
    "          'TH': np.zeros((len(depth),len(lat)))}\n",
    "comp_grid = np.zeros_like(t_grid['TC'])\n",
    "\n",
    "for filename in glob.glob(os.path.join(input_path, 'D12-TC_4000/*.dat')):\n",
    "    loc = int(os.path.splitext(filename)[0].split('_')[-1]) - 1\n",
    "    if loc >= 14:\n",
    "        data1 = np.loadtxt(filename)\n",
    "        loc -= 14\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    t_grid['TC'][loc] = data1[:,0]\n",
    "    comp_grid[loc] = data1[:,2]\n",
    "    i += 1\n",
    "\n",
    "for filename in glob.glob(os.path.join(input_path, 'D12-TH_4000/*.dat')):\n",
    "    loc = int(os.path.splitext(filename)[0].split('_')[-1]) - 1\n",
    "    if loc >= 14:\n",
    "        data1 = np.loadtxt(filename)\n",
    "        loc -= 14\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    t_grid['TH'][loc] = data1[:,0]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create properties for compositions and ppv\n",
    "def find_comp_properties(comp):\n",
    "    comp_all = ['pyrolite', 'pyroliteTC', 'BMO', 'MORB', 'HC']\n",
    "    comp_index = comp_all.index(comp)\n",
    "    t_grid = ['TH', 'TC', 'TC', 'TC', 'TC']\n",
    "    X = [None, None, comp_grid, comp_grid, comp_grid]\n",
    "\n",
    "    return t_grid[comp_index], X[comp_index]\n",
    "\n",
    "def find_ppv_type(ppv):\n",
    "    ppv_type = ['none', 'two_phase', None]\n",
    "    ppv_all = ['noppv', 'ppv', 'partppv']\n",
    "    ppv_index = ppv_all.index(ppv)\n",
    "\n",
    "    return ppv_type[ppv_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TwoPhaseRegion Objects\n",
    "save = True\n",
    "load = True\n",
    "\n",
    "phaseboundary = {}\n",
    "for min_model in min_model_list:\n",
    "    for comp in comp_list:\n",
    "        name = f'{comp}_{min_model[-2:]}'\n",
    "        if load:\n",
    "            phaseboundary[name] = gc.BdgPPvTwoPhaseRegion.from_txt(os.path.join(input_path, f'ppv_two_phase_boundary_{name}'))\n",
    "        else:\n",
    "            phaseboundary[name] = gc.BdgPPvTwoPhaseRegion(comp, min_model=min_model, save=save, outdir=input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Oxide to Phase\n",
    "save = True\n",
    "load = True\n",
    "\n",
    "phases = {}\n",
    "for min_model in min_model_list:\n",
    "    for comp in ['pyroliteTC'] + comp_list:  \n",
    "        name = f'{comp}_{min_model[-2:]}'\n",
    "        t, X_type = find_comp_properties(comp)\n",
    "        if load:\n",
    "            filename = os.path.join(input_path, f'phases_{comp}_{min_model[-2:]}.npz')\n",
    "            phases[name] = gc.PhaseGrid(filename, t_grid[t], depth, lon, lat, comp, min_model)\n",
    "\n",
    "        else:\n",
    "            phases[name] = gc.oxide_to_phase(t_grid[t], depth, lon, lat, comp, phaseboundary[name], X=X_type, min_model=min_model, save=save, outdir=input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Elastic Parameters and create ElasticGrids\n",
    "save = True    \n",
    "\n",
    "elastic = {}\n",
    "for min_model in min_model_list:\n",
    "    for comp in ['pyroliteTC'] + comp_list:  \n",
    "        phase_name = f'{comp}_{min_model[-2:]}'\n",
    "        _, X_type = find_comp_properties(comp)\n",
    "\n",
    "        for ppv in ppv_list:\n",
    "            ppv_type = find_ppv_type(ppv)\n",
    "            elastic_name = f'{comp}_{ppv}_{min_model[-2:]}'\n",
    "\n",
    "            if ppv == 'partppv':\n",
    "                elastic[elastic_name] = gc.ElasticGrid.from_file(input_path, comp, 'partppv', depth, lon, lat, min_model=min_model, comp_grid=comp_grid)\n",
    "            elif load:\n",
    "                elastic[elastic_name] = gc.ElasticGrid.from_file(input_path, comp, ppv, depth, lon, lat, min_model=min_model)\n",
    "            else:\n",
    "                if X_type is not None:\n",
    "                    py_model = elastic[f'pyroliteTC_{ppv}_{min_model[-2:]}']\n",
    "                else:\n",
    "                    py_model = None\n",
    "                elastic[elastic_name] = phases[phase_name].evaluate_elastic(ppv_type, X=X_type, py_model=py_model, save=save, outdir=input_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ElasticGrids\n",
    "for min_model in min_model_list:\n",
    "    for comp in comp_list:\n",
    "        for ppv in ppv_list:\n",
    "            if ppv == 'partppv':\n",
    "                X = comp_grid\n",
    "            else:\n",
    "                X = None\n",
    "            name = f'{comp}_{ppv}_{min_model[-2:]}'\n",
    "            elastic{name} = gc.ElasticGrid.from_file(input_path, comp, ppv, depth, lon, lat, min_model=min_model, comp_grid=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spherical harmonics and create/load RawSeismicModels\n",
    "save = True\n",
    "load = True\n",
    "spherical_degree = 8\n",
    "radial_degree = 20 \n",
    "\n",
    "raw_velocity = {}\n",
    "for min_model in min_model_list:\n",
    "    for comp in comp_list:\n",
    "        for ppv in ppv_list:\n",
    "            name = f'{comp}_{ppv}_{min_model[-2:]}'\n",
    "            if load:\n",
    "                raw_velocity{name} = gc.RawSeismicModel.from_file(radial_degree, input_path, \n",
    "                                                                  comp, ppv, min_model, seismic_model='SOLA_')\n",
    "            else:\n",
    "                raw_velocity{name} = elastic{name}.to_continuous_param(r_deg=radial_degree, sph_deg=spherical_degree, \n",
    "                                                                       save=save, outdir=input_path, \n",
    "                                                                       filename=f'SOLA_{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Filtered Seismic Models\n",
    "\n",
    "models = {}\n",
    "for min_model in min_model_list:\n",
    "    for comp in comp_list:\n",
    "        for ppv in ppv_list:\n",
    "            name = f'{comp}_{ppv}_{min_model[-2:]}'\n",
    "            models{name} = raw_velocity{name}.to_SOLA()\n",
    "\n",
    "            models{name}.apply_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate distributions of tomographic characteristics\n",
    "Here, we obtain distributions of RMS $d\\ln V_p$, RMS $d\\ln V_s$, $R_{s/p}$, $r_{sâ€“c}$ to get uncertainties of these tomographic characteristics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to calculate R_s/p and r_s-c\n",
    "def rms(array, ax = None, data_type = None):\n",
    "    \"\"\"\n",
    "    Calculates the root mean square (RMS) of values in an array.\n",
    "    If the array is a set of spherical coefficients, set \n",
    "    data_type = 'SH'.\n",
    "    \"\"\"\n",
    "    if data_type == \"SH\":\n",
    "        return np.sqrt(np.sum((array ** 2) / (4 * np.pi), axis = ax))\n",
    "    else:\n",
    "        return np.sqrt(np.sum((array ** 2) / np.size(array), axis = ax))\n",
    "    \n",
    "    \n",
    "def SOLA_correlate(v1, v2, deg, omit_zero = False):\n",
    "    \"\"\"\n",
    "    Correlates two sets of spherical harmonics v1 and v2.\n",
    "    \"\"\"\n",
    "\n",
    "    corr = []\n",
    "\n",
    "    if omit_zero:\n",
    "        f = 1\n",
    "    else:\n",
    "        f = 0\n",
    "\n",
    "    # loop through each layer, correlation assumes lower resolution of the two\n",
    "    cross = shtools.spectralanalysis.cross_spectrum(v1, v2,\n",
    "                            degrees=deg,\n",
    "                            normalization='ortho')\n",
    "    a = shtools.spectralanalysis.spectrum(v1, \n",
    "                            degrees=deg,\n",
    "                            normalization='ortho')\n",
    "    b = shtools.spectralanalysis.spectrum(v2,\n",
    "                            degrees=deg,\n",
    "                            normalization='ortho')\n",
    "    corr = np.sum(cross[f:])/np.sqrt(np.sum(a[f:])*np.sum(b[f:]))    \n",
    "    \n",
    "    return corr \n",
    "\n",
    "\n",
    "def calculate_R(dvs, dvp, lat, lon, threshold = 0.1, deg = None):\n",
    "    \"\"\"\n",
    "    Calculate the average $R_{s/p} = d \\ln V_s / d\\ln V_p$ over a depth slice by two methods: \n",
    "    root mean square in grid space or by spherical harmonics.\n",
    "    \"\"\"\n",
    "    SH = rms(dvs, data_type='SH')/rms(dvp, data_type='SH')\n",
    "\n",
    "    grid_vs = make_grids_with_deg_filter(dvs, lat, lon, deg = deg)\n",
    "    grid_vp = make_grids_with_deg_filter(dvp, lat, lon, deg = deg)\n",
    "\n",
    "    root = rms(grid_vs[(abs(grid_vs) > threshold) & (abs(grid_vp) > threshold)])/rms(grid_vp[(abs(grid_vs) > threshold) & (abs(grid_vp) > threshold)])\n",
    "\n",
    "    return root, SH\n",
    "\n",
    "\n",
    "def make_grids_with_deg_filter(coefs, lat, lon, deg = None):\n",
    "    \"\"\"\n",
    "    Transform spherical coefficients (coefs) to real space, as determined by a list of latitudes (lat)\n",
    "    and longitudes (lon).\n",
    "    \"\"\"\n",
    "    new_coefs = np.zeros_like(coefs)\n",
    "\n",
    "    if deg != None:\n",
    "        for d in deg:\n",
    "            new_coefs[:,d] = coefs[:,d]\n",
    "    else:\n",
    "        new_coefs = coefs\n",
    "\n",
    "    return shtools.expand.MakeGridPoint(new_coefs, lat, lon, norm = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SOLA Tomographic Model\n",
    "SOLA_path = \"../data/SOLA_model/\"\n",
    "SOLA_model = gc.SOLAShell.from_directory(SOLA_path)\n",
    "SOLA_model.apply_resolving_kernel()\n",
    "\n",
    "SOLA_model.vp = tomoproxy.sh_tools.rts_to_sh(SOLA_model.vp) / 100\n",
    "SOLA_model.vs = tomoproxy.sh_tools.rts_to_sh(SOLA_model.vs) / 100\n",
    "SOLA_model.vphi = tomoproxy.sh_tools.rts_to_sh(SOLA_model.vphi) / 100\n",
    "SOLA_model.vp_err = tomoproxy.sh_tools.rts_to_sh(SOLA_model.vp_err) / 100\n",
    "SOLA_model.vs_err = tomoproxy.sh_tools.rts_to_sh(SOLA_model.vs_err) / 100\n",
    "SOLA_model.vphi_err = tomoproxy.sh_tools.rts_to_sh(SOLA_model.vphi_err) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save Uncertainties on R and Vs-Vc with SH\n",
    "save = True\n",
    "load = True\n",
    "\n",
    "if load:\n",
    "    a = np.load(input_path + 'SOLA_model_realisations.npz',)\n",
    "    SOLA_Vp_layer_rand = a['vp_random']\n",
    "    SOLA_Vs_layer_rand = a['vs_random']\n",
    "    SOLA_Vphi_layer_rand = a['vphi_random']\n",
    "    R_random = a['R_random']\n",
    "    corr_random = a['corr_random']\n",
    "\n",
    "    Vp_random = rms(SOLA_Vp_layer_rand, ax = (1,2,3), data_type = 'SH')\n",
    "    Vs_random = rms(SOLA_Vs_layer_rand, ax = (1,2,3), data_type = 'SH')\n",
    "    Vphi_random = rms(SOLA_Vphi_layer_rand, ax = (1,2,3), data_type = 'SH')\n",
    "else:\n",
    "    np.random.seed(12345)\n",
    "    SOLA_Vp_layer_rand = np.random.normal(SOLA_model.vp, np.abs(SOLA_model.vp_err), (1000000,2,9,9))\n",
    "\n",
    "    np.random.seed(93720)\n",
    "    SOLA_Vs_layer_rand = np.random.normal(SOLA_model.vs, np.abs(SOLA_model.vs_err), (1000000,2,9,9))\n",
    "\n",
    "    np.random.seed(36183)\n",
    "    SOLA_Vphi_layer_rand = np.random.normal(SOLA_model.vphi, np.abs(SOLA_model.vphi_err), (1000000,2,9,9))\n",
    "\n",
    "    Vp_random = rms(SOLA_Vp_layer_rand, ax = (1,2,3), data_type= = 'SH')\n",
    "    Vs_random = rms(SOLA_Vs_layer_rand, ax = (1,2,3), data_type = 'SH')\n",
    "    Vphi_random = rms(SOLA_Vphi_layer_rand, ax = (1,2,3), data_type = 'SH')\n",
    "    R_random = Vs_random/Vp_random\n",
    "    corr_random = [SOLA_correlate(SOLA_Vphi_layer_rand[i],SOLA_Vs_layer_rand[i], deg = np.arange(0,9,1), omit_zero = True) for i in range(1000000)]\n",
    "\n",
    "    if save:\n",
    "        np.savez(os.path.join(input_path, 'SOLA_model_realisations'),\n",
    "                vp_random = SOLA_Vp_layer_rand,\n",
    "                vs_random = SOLA_Vs_layer_rand,\n",
    "                vphi_random = SOLA_Vphi_layer_rand,\n",
    "                R_random = R_random,\n",
    "                corr_random = corr_random\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import equidistant grid\n",
    "eq_dist = np.loadtxt('../deps/see-rts-filtering/data_files/SP12RTS.1x1/slices.eqdst.1/SP12RTS..EC.0025.eqdst.1.latlon.dat', usecols = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate R (only run if files are not downloaded)\n",
    "root = np.zeros((24, 9))\n",
    "SH = np.zeros_like(root)\n",
    "\n",
    "save = True\n",
    "load = True\n",
    "\n",
    "if load:\n",
    "    a = np.load(input_path + 'R_values.npz')\n",
    "    root = a['root']\n",
    "    SH = a['SH']\n",
    "else:\n",
    "    for i, m in enumerate(models.values()):\n",
    "        root[i, 0], SH[i, 0] = calculate_R(m.vs, m.vp, eq_dist[:,0], eq_dist[:,1])\n",
    "        for j in range(1,9):\n",
    "            root[i, j], SH[i, j] = calculate_R(m.vs, m.vp, eq_dist[:,0], eq_dist[:,1], deg = [j])\n",
    "\n",
    "    if save:\n",
    "        np.savez(input_path + 'R_values',\n",
    "                root = root,\n",
    "                SH = SH\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate S-C correlation (only run if files are not downloaded)\n",
    "save = True\n",
    "load = True\n",
    "\n",
    "if load:\n",
    "    a = np.load(input_path + 'vphi_vs_corr.npz')\n",
    "    vphi_vs_corr = a['vphi_vs_corr']\n",
    "else:\n",
    "    vphi_vs_corr = np.zeros((24, 9))\n",
    "    for i, m in enumerate(models):\n",
    "        vphi_vs_corr[i,0] = SOLA_correlate(m.vphi, m.vs, np.arange(0,9,1), omit_zero = True)\n",
    "        for j in range(1,9):\n",
    "            vphi_vs_corr[i,j] = SOLA_correlate(m.vphi, m.vs, [j], omit_zero = False)\n",
    "    if save:\n",
    "        np.savez(input_path + 'vphi_vs_corr',\n",
    "                vphi_vs_corr = vphi_vs_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cs/4dd1zds94b5c7zcgwb3dbv3c0000gq/T/ipykernel_78040/3716363753.py:20: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  root = rms(grid_vs[(abs(grid_vs) > threshold) & (abs(grid_vp) > threshold)])/rms(grid_vp[(abs(grid_vs) > threshold) & (abs(grid_vp) > threshold)])\n",
      "/var/folders/cs/4dd1zds94b5c7zcgwb3dbv3c0000gq/T/ipykernel_78040/3867904830.py:39: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  root_terra[i,k] = rms(t_s[(abs(t_s) > threshold) & (abs(t_p) > threshold)]) / rms(t_p[(abs(t_s) > threshold) & (abs(t_p) > threshold)])\n",
      "/Users/univ4732/anaconda3/envs/lema/lib/python3.9/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/univ4732/anaconda3/envs/lema/lib/python3.9/site-packages/numpy/lib/function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "# For original and reparameterised models\n",
    "load = True\n",
    "save = True\n",
    "\n",
    "if load:\n",
    "    a = np.load(input_path + 'R_and_corr_terra_and_reparam.npz',)\n",
    "    root_terra = a['root_terra']\n",
    "    vphi_vs_corr_terra = a['vphi_vs_corr_terra']\n",
    "else:\n",
    "    depths = 6371 - depth[-20:]\n",
    "    threshold = 0.1\n",
    "    root_terra = np.zeros((24, len(depths)))\n",
    "    root_reparam = np.zeros((24,len(depths), 9))\n",
    "    SH_reparam = np.zeros_like(root_reparam)\n",
    "\n",
    "    vphi_vs_corr_terra = np.zeros((24, len(depths)))\n",
    "    vphi_vs_corr_reparam = np.zeros((24, len(depths), 9))\n",
    "\n",
    "    comp_list = ['pyrolite', 'BMO', 'MORB', 'HC']\n",
    "    ppv_list = ['noppv', 'ppv', 'partppv']\n",
    "    min_model_list = ['SLB_2022', 'SLB_2011']\n",
    "\n",
    "\n",
    "    for min_model in min_model_list:\n",
    "        for comp in comp_list:  \n",
    "            for ppv in ppv_list:\n",
    "                name = f'{comp}_{ppv}_{min_model[-2:]}'\n",
    "                \n",
    "        terra_model = elastic[name]\n",
    "        sshell = raw_velocity[name]\n",
    "\n",
    "        for k, d in enumerate(depths):\n",
    "            index = -20 + k\n",
    "            vs_shell = sshell.vs.get_sh_coefs_at_r(d)\n",
    "            vp_shell = sshell.vp.get_sh_coefs_at_r(d)\n",
    "            vphi_shell = sshell.vphi.get_sh_coefs_at_r(d)\n",
    "            t_s = 100*(terra_model.vs_grid[index] - vs_shell[0,0,0] / (2.0 * np.sqrt(np.pi))) / (vs_shell[0,0,0] / (2.0 * np.sqrt(np.pi)))\n",
    "            t_p = 100*(terra_model.vp_grid[index] - vp_shell[0,0,0] / (2.0 * np.sqrt(np.pi))) / (vp_shell[0,0,0] / (2.0 * np.sqrt(np.pi)))\n",
    "            t_phi = 100*(terra_model.vphi_grid[index] - vphi_shell[0,0,0] / (2.0 * np.sqrt(np.pi))) / (vp_shell[0,0,0] / (2.0 * np.sqrt(np.pi)))\n",
    "            vs_shell /= vs_shell[0,0,0] / (2.0 * np.sqrt(np.pi))\n",
    "            vs_shell[0,0,0] = 0\n",
    "            vp_shell /= vp_shell[0,0,0] / (2.0 * np.sqrt(np.pi))\n",
    "            vp_shell[0,0,0] = 0\n",
    "            vphi_shell /= vphi_shell[0,0,0] / (2.0 * np.sqrt(np.pi))\n",
    "            vphi_shell[0,0,0] = 0\n",
    "\n",
    "            root_terra[i,k] = rms(t_s[(abs(t_s) > threshold) & (abs(t_p) > threshold)]) / rms(t_p[(abs(t_s) > threshold) & (abs(t_p) > threshold)])\n",
    "            vphi_vs_corr_terra[i,k] = np.corrcoef(t_s, t_phi)[0,1]\n",
    "\n",
    "            root_reparam[i, k, 0], SH_reparam[i,k, 0] = calculate_R(100*vs_shell, 100*vp_shell, eq_dist[:,0], eq_dist[:,1])\n",
    "            vphi_vs_corr_reparam[i,k,0] = SOLA_correlate(100*vphi_shell, 100*vs_shell, np.arange(0,9,1), omit_zero = True)\n",
    "\n",
    "            for j in range(1,9):\n",
    "                root_reparam[i, k, j], SH_reparam[i, k, j] = calculate_R(100*vs_shell, 100*vp_shell, eq_dist[:,0], eq_dist[:,1], deg = [j])\n",
    "                vphi_vs_corr_reparam[i,k,j] = SOLA_correlate(100*vphi_shell, 100*vs_shell, [j], omit_zero = False)\n",
    "        \n",
    "    if save:\n",
    "        np.savez(input_path + 'R_and_corr_terra_and_reparam',\n",
    "                root_terra = root_terra,\n",
    "                root_reparam = root_reparam,\n",
    "                SH_reparam = SH_reparam,\n",
    "                vphi_vs_corr_terra = vphi_vs_corr_terra,\n",
    "                vphi_vs_corr_reparam = vphi_vs_corr_reparam)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
